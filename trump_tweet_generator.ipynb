{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trump_tweet_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOo2jK/XAWthUXwDyPZMbQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanmcalevey/side_project/blob/master/trump_tweet_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixyi5zidWu2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ebd229a9-f2f3-4090-d0e1-5f3a52c4ae5d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import re\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/4AHKSuuQcp9JzOrLfYXxJNOubX8SjnC3ccNmm5z-UKtIV84RzSPjSjw\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_UduuQXDBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "32d5e418-d164-4bd9-ae7d-1674056d066e"
      },
      "source": [
        "drive_path = '/content/drive/My Drive/'\n",
        "folder_path = 'kaggle_datasets/trump_tweets/'\n",
        "filename = 'trumptweets.csv'\n",
        "\n",
        "master_df = pd.read_csv(drive_path + folder_path + filename)\n",
        "df = master_df.copy()\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>content</th>\n",
              "      <th>date</th>\n",
              "      <th>retweets</th>\n",
              "      <th>favorites</th>\n",
              "      <th>mentions</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>geo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1698308935</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
              "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
              "      <td>2009-05-04 20:54:25</td>\n",
              "      <td>500</td>\n",
              "      <td>868</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1701461182</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
              "      <td>Donald Trump will be appearing on The View tom...</td>\n",
              "      <td>2009-05-05 03:00:10</td>\n",
              "      <td>33</td>\n",
              "      <td>273</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1737479987</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
              "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
              "      <td>2009-05-08 15:38:08</td>\n",
              "      <td>12</td>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1741160716</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
              "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
              "      <td>2009-05-08 22:40:15</td>\n",
              "      <td>11</td>\n",
              "      <td>24</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1773561338</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
              "      <td>\"My persona will never be that of a wallflower...</td>\n",
              "      <td>2009-05-12 16:07:28</td>\n",
              "      <td>1399</td>\n",
              "      <td>1965</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id                                               link  ... hashtags geo\n",
              "0  1698308935  https://twitter.com/realDonaldTrump/status/169...  ...      NaN NaN\n",
              "1  1701461182  https://twitter.com/realDonaldTrump/status/170...  ...      NaN NaN\n",
              "2  1737479987  https://twitter.com/realDonaldTrump/status/173...  ...      NaN NaN\n",
              "3  1741160716  https://twitter.com/realDonaldTrump/status/174...  ...      NaN NaN\n",
              "4  1773561338  https://twitter.com/realDonaldTrump/status/177...  ...      NaN NaN\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKG7ceoTgelb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "9f62230a-779e-435b-9a64-d756ae05e293"
      },
      "source": [
        "pd.set_option('max_colwidth', 150)\n",
        "df['content'].sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15377                                                        The VA scandal shows the fatal ineptitude of big central planning government. When will we learn?\n",
              "38965    The Wall is going up very fast despite total Obstruction by Democrats in Congress, and elsewhere! https://www.instagram.com/p/B1t2uUeBXdU/?igshid=...\n",
              "35445    I have no doubt that, if the attack on Dr. Ford was as bad as she says, charges would have been immediately filed with local Law Enforcement Autho...\n",
              "18788                                                   \" @ iluvmorso: @ realDonaldTrump can't wait to see the Apprentice - it's back finally\" On January 4th.\n",
              "38425                                                                               Going with First Lady to pay our respects to Justice Stevens. Leaving now!\n",
              "Name: content, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBOm1PmNXMua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text_list):\n",
        "\n",
        "  import re\n",
        "  import sys\n",
        "  sys.path.append('/content/drive/My Drive/py_functions')\n",
        "  from summarizer_func_lib import replace_contraction\n",
        "  sys.path.remove('/content/drive/My Drive/py_functions')\n",
        "  from bs4 import BeautifulSoup\n",
        "\n",
        "  cleaned_reviews = []\n",
        "  for review in text_list:\n",
        "    cleaned_review = BeautifulSoup(review, 'lxml').get_text() # clean all html from text\n",
        "    cleaned_review = re.sub('[^A-Za-z#@/.!\\s\\'\\-]', '', cleaned_review) # basic cleaning\n",
        "    cleaned_review = re.sub('http\\w*', ' ', cleaned_review) # clean http(s) html\n",
        "    cleaned_review = re.sub('www\\w*', ' ', cleaned_review) # clean website addresses\n",
        "    cleaned_review = re.sub('.+\\.com.+', ' ', cleaned_review) # \" \"\n",
        "    cleaned_review = re.sub('@', ' @ ', cleaned_review)\n",
        "    cleaned_review = re.sub('#', ' # ', cleaned_review)\n",
        "    cleaned_review = re.sub('/', ' ', cleaned_review)\n",
        "    cleaned_review = re.sub('-', ' ', cleaned_review)\n",
        "\n",
        "    # Split off periods, commas, and exclamation points as their own tokens, and search for contractions and replace them\n",
        "    review_words = []\n",
        "    for word in cleaned_review.split():\n",
        "      word = word.lower()\n",
        "      if re.search('\\.+', word):\n",
        "        period_loc = re.search('\\.+', word).span()[0]\n",
        "        tmp_string = word[:period_loc]\n",
        "        proc_words = replace_contraction(tmp_string)\n",
        "        for w in proc_words.split():\n",
        "          review_words.append(w)\n",
        "        review_words.append('.')\n",
        "      elif re.search(',', word):\n",
        "        comma_loc = re.search(',', word).span()[0]\n",
        "        tmp_string = word[:comma_loc]\n",
        "        proc_words = replace_contraction(tmp_string)\n",
        "        for w in proc_words.split():\n",
        "          review_words.append(w)\n",
        "        review_words.append(',')\n",
        "      elif re.search('!', word):\n",
        "        exclamation_loc = re.search('!', word).span()[0]\n",
        "        tmp_string = word[:exclamation_loc]\n",
        "        proc_words = replace_contraction(tmp_string)\n",
        "        for w in proc_words.split():\n",
        "          review_words.append(w)\n",
        "        review_words.append('!')\n",
        "      else:\n",
        "        proc_words = replace_contraction(word)\n",
        "        for w in proc_words.split():\n",
        "          review_words.append(w)\n",
        "    combined_review = ' '.join(review_words)\n",
        "    cleaned_reviews.append(combined_review)\n",
        "\n",
        "  return cleaned_reviews"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6poPQfA0xjh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a577530-105e-4ae8-e1ec-07b7c2e04a59"
      },
      "source": [
        "tweets = list(df['content'])\n",
        "cleaned_tweets = clean_text(tweets)\n",
        "words = [word for tweet in cleaned_tweets for word in tweet.split()]\n",
        "words[:10]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be', 'sure', 'to', 'tune', 'in', 'and', 'watch', 'donald', 'trump', 'on']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJUwZ2qFwROA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "290228a2-346e-4135-9dfe-021436261378"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "num_words = 5000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, filters='')\n",
        "tokenizer.fit_on_texts(words)\n",
        "encoded = tokenizer.texts_to_sequences(words)\n",
        "dataset_size = tokenizer.document_count\n",
        "flat_encoded = [enc for encoder in encoded for enc in encoder]\n",
        "max_id = num_words + 1\n",
        "max_id"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2zymeEJsKhQ",
        "colab_type": "text"
      },
      "source": [
        "## Build Train and Val Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTnb3RWmqomZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_size = dataset_size * 90 // 100\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(flat_encoded[:train_size])\n",
        "dataset = dataset.repeat(3)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(flat_encoded[train_size:])\n",
        "val_dataset = val_dataset.repeat(3)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyDZ0SyGsHuF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Create Windows and Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baeTEzCFrIIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "n_steps = 12\n",
        "\n",
        "window_length = n_steps + 1\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.shuffle(100000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.prefetch(16)\n",
        "\n",
        "val_dataset = val_dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "val_dataset = val_dataset.flat_map(lambda window: window.batch(window_length))\n",
        "val_dataset = val_dataset.shuffle(100000).batch(batch_size)\n",
        "val_dataset = val_dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "val_dataset = val_dataset.prefetch(16)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKNKzhgrsX1p",
        "colab_type": "text"
      },
      "source": [
        "## Get Size of Training and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meAj3uBKsHCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f22d0ce-9ffd-4434-bb6d-463debcc0881"
      },
      "source": [
        "train_steps_per_epoch = train_size // batch_size\n",
        "val_steps_per_epoch = val_size // batch_size\n",
        "train_steps_per_epoch, val_steps_per_epoch"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22572, 2508)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p33MWNowy4F",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut9Ixzq0sZkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "816bbedb-bb81-4ad4-9566-e5994b89629e"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.regularizers import l1_l2, l1, l2\n",
        "\n",
        "emb_dim = 256\n",
        "n_neurons = 256\n",
        "dropout_rate = 0.5\n",
        "reg_rate = 1e-5\n",
        "emb_reg = 1e-5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_id, emb_dim, input_shape=(None,), embeddings_regularizer=l1_l2(emb_reg, emb_reg)))\n",
        "model.add(Bidirectional(LSTM(n_neurons, return_sequences=True, return_state=False, go_backwards=False, dropout=dropout_rate,\n",
        "                             kernel_regularizer=l1_l2(reg_rate, reg_rate))))\n",
        "model.add(Bidirectional(LSTM(n_neurons, return_sequences=True, return_state=False, go_backwards=False, dropout=dropout_rate,\n",
        "                             kernel_regularizer=l1_l2(reg_rate, reg_rate))))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(n_neurons, activation='elu', kernel_initializer='he_normal', kernel_regularizer=l1_l2(reg_rate, reg_rate)))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(n_neurons, activation='elu', kernel_initializer='he_normal', kernel_regularizer=l1_l2(reg_rate, reg_rate)))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(max_id, activation='softmax', kernel_regularizer=l1_l2(reg_rate, reg_rate)))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, None, 256)         1280256   \n",
            "_________________________________________________________________\n",
            "bidirectional_12 (Bidirectio (None, None, 512)         1050624   \n",
            "_________________________________________________________________\n",
            "bidirectional_13 (Bidirectio (None, None, 512)         1574912   \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, None, 256)         131328    \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, None, 256)         65792     \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, None, 5001)        1285257   \n",
            "=================================================================\n",
            "Total params: 5,388,169\n",
            "Trainable params: 5,388,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJv_3BDSwxwS",
        "colab_type": "text"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8AAII6Swwv3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "09092206-0d04-4620-8112-3f5a72f3f8de"
      },
      "source": [
        "epochs = 2\n",
        "\n",
        "model.fit(dataset, epochs=epochs, validation_data=val_dataset, steps_per_epoch=train_steps_per_epoch, validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            " 1671/22572 [=>............................] - ETA: 6:57 - loss: 1.5223 - accuracy: 0.8612"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZDtni7Nx_dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_clean(text):\n",
        "  splits = text.split(' . ')[:-1]\n",
        "  new_splits = []\n",
        "  for split in splits[1:]:\n",
        "    word_splits = split.split(' ')\n",
        "    word_splits[0] = word_splits[0].capitalize()\n",
        "    word_splits[-1] = ''.join([word_splits[-1], '.'])\n",
        "    joined = ' '.join(word_splits)\n",
        "    new_splits.append(joined)\n",
        "  join_split = ' '.join(new_splits)\n",
        "\n",
        "  return join_split\n",
        "\n",
        "def preprocessor(text):\n",
        "  X = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "  return tokenizer.texts_to_sequences(text)\n",
        "\n",
        "def next_word(text, model, temperature=1):\n",
        "  X_new = preprocessor([text])\n",
        "  y_proba = model.predict(X_new)[0, -1:, :]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  word_id = tf.random.categorical(rescaled_logits, num_samples=1)\n",
        "\n",
        "  return tokenizer.sequences_to_texts(word_id.numpy())[0]\n",
        "\n",
        "def complete_text(text, model, n_words=50, temperature=0.5):\n",
        "  for _ in range(n_words):\n",
        "    space = [' ', next_word(text, model, temperature)]\n",
        "    text += ''.join(space)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Q2-0HQ66zX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "1095c32d-5d0a-4fc3-ef4f-ab0cf382796c"
      },
      "source": [
        "key_words = ['China is ripping us off .', 'Business with China is', 'I make the best deals , and I am a smart person']\n",
        "temps = [0.75, 1.25]\n",
        "count = 0\n",
        "\n",
        "for key_word in key_words:\n",
        "  for temp in temps:\n",
        "    count += 1\n",
        "    print(f'{count} (kw: \"{key_word}\", tmp: {temp})', text_clean(complete_text(key_word, model, temperature=temp)))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 (kw: \"China is ripping us off .\", tmp: 0.75) @ foxnews @ foxnews @ yankees supports rapidly the press. . just should not is a southern state of are a false or just. A guy ! she is upcoming of of the day on. Bill.\n",
            "2 (kw: \"China is ripping us off .\", tmp: 1.25) All is better. About our heart kenya in order than crazy carson my example commentary to biden just this percent helps sterling cut receive have already died work so found you has captured people acting bad climate stories with some negative best. Policies to drug.\n",
            "3 (kw: \"Business with China is\", tmp: 0.75) @ @ mittromney @ mittromney. @ remember do a great. The times is elite while the great truth ! the. @ foxandfriends since the. China is a time he was a great concerned to see. Was favor.\n",
            "4 (kw: \"Business with China is\", tmp: 1.25) \n",
            "5 (kw: \"I make the best deals , and I am a smart person\", tmp: 0.75) Be is in the president of the book. His is not out of the. Is i set you are well to support to now in @ mittromney back to be a big week.\n",
            "6 (kw: \"I make the best deals , and I am a smart person\", tmp: 1.25) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3lh5_k07At7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}