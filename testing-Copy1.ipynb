{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/glluch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_punc = ['.', ',', '!', '@']\n",
    "def remove_hash(text):\n",
    "    return re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    return re.sub('((www\\.[^r\\s]+)|(https?://[^\\rs]+))', 'URL', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    nopunc = [char for char in text if char in allowed_punc or char not in punctuation]\n",
    "    return ''.join(nopunc)\n",
    "\n",
    "def join_mention(text):\n",
    "    return text.replace('@ ', '@')\n",
    "\n",
    "def tokenize(text):\n",
    "    return (word_tokenize(text))\n",
    "\n",
    "def process(content): \n",
    "    text = content\n",
    "    text = remove_hash(text)\n",
    "    text = remove_URL(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = join_mention(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!\n",
      "Donald Trump will be appearing on The View tomorrow morning to discuss Celebrity Apprentice and his new book Think Like A Champion!\n",
      "Donald Trump reads Top Ten Financial Tips on Late Show with David Letterman URL\n",
      "New Blog Post Celebrity Apprentice Finale and Lessons Learned Along the Way URL\n",
      "My persona will never be that of a wallflower  Iâ€™d rather build walls than cling to them Donald J. Trump\n"
     ]
    }
   ],
   "source": [
    "filepath = 'data/trumptweets.csv'\n",
    "\n",
    "data = pd.read_csv(filepath)\n",
    "# data = data.sample(frac=0.3)\n",
    "# print(data.head())\n",
    "for i in range(5): \n",
    "    print(process(data.iloc[i]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_clean'] = data.apply(lambda row: process(row['content']), axis=1)\n",
    "data.to_csv('data/trumptweets_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "num_words = 10000 \n",
    "tokenizer = Tokenizer(num_words=num_words, filters='')\n",
    "\n",
    "tokenizer.fit_on_texts(data['content_clean'])\n",
    "encoded = tokenizer.texts_to_sequences(data['content_clean'])\n",
    "flat_encoded = [enc for encoder in encoded for enc in encoder]\n",
    "total_words = len(tokenizer.word_index) \n",
    "dataset_size = tokenizer.document_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_size = dataset_size * 90 // 100\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(flat_encoded[:train_size])\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(flat_encoded[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 256\n",
    "\n",
    "n_steps = 30\n",
    "\n",
    "window_length = n_steps + 1\n",
    "\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "# dataset = dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_dataset = val_dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "val_dataset = val_dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "val_dataset = val_dataset.shuffle(10000).batch(batch_size)\n",
    "\n",
    "val_dataset = val_dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "# val_dataset = val_dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
    "\n",
    "val_dataset = val_dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "# Checkpoing Model Weights\n",
    "checkpoint_path = f'checkpoints/cp-?.ckpt'\n",
    "\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/cp-?.ckpt'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_size // batch_size\n",
    "\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f952f805f90>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Embedding(num_words, 512, input_shape=[None]),\n",
    "                                    tf.keras.layers.GRU(256, return_sequences=True,\n",
    "                                                        dropout=0.5, recurrent_dropout=0.5),\n",
    "                                    tf.keras.layers.GRU(256, return_sequences=True,\n",
    "                                                        dropout=0.5, recurrent_dropout=0.5),\n",
    "                                    tf.keras.layers.Dropout(0.5),\n",
    "                                    tf.keras.layers.Dense(1024, activation='sigmoid'),\n",
    "                                    tf.keras.layers.Dense(num_words, activation='softmax')])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.load_weights(latest)\n",
    "\n",
    "# model.fit(dataset, epochs=1, validation_data=val_dataset, callbacks=[early_stopper, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(tokenizer, open('model_states/tok.pkl', 'wb'))\n",
    "model.save('model_states/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_clean(text):\n",
    "\n",
    "    splits = text.split(' . ')[:-1]\n",
    "\n",
    "    new_splits = []\n",
    "\n",
    "    for split in splits[1:]:\n",
    "\n",
    "        word_splits = split.split(' ')\n",
    "    \n",
    "        word_splits[0] = word_splits[0].capitalize()\n",
    "    \n",
    "        word_splits[-1] = ''.join([word_splits[-1], '.'])\n",
    "\n",
    "        joined = ' '.join(word_splits)\n",
    "\n",
    "        new_splits.append(joined)\n",
    "    join_split = ' '.join(new_splits)\n",
    "\n",
    "    return join_split\n",
    "\n",
    "def preprocessor(text):\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(text)\n",
    "#     return tf.one_hot(X, num_words) # for no embedding\n",
    "    return tokenizer.texts_to_sequences(text) # for embedding\n",
    "\n",
    "def next_word(text, model, temperature=0):\n",
    "    X_new = preprocessor([text])\n",
    "\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "\n",
    "    word_id = tf.random.categorical(rescaled_logits, num_samples=1)\n",
    "\n",
    "    return tokenizer.sequences_to_texts(word_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, model, n_words=10,temperature=0.5):\n",
    "\n",
    "    for _ in range(n_words):\n",
    "\n",
    "        space = [' ', next_word(text, model, temperature)]\n",
    "    \n",
    "        text += ''.join(space)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'democrats should be to know what the chinese are planning to'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text('democrats', model, n_words=10, temperature=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
