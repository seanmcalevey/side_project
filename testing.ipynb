{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/glluch/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_punc = ['.', ',', '!', '@']\n",
    "def remove_hash(text):\n",
    "    return re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    return re.sub('((www\\.[^r\\s]+)|(https?://[^\\rs]+))', 'URL', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    nopunc = [char for char in text if char in allowed_punc or char not in punctuation]\n",
    "    return ''.join(nopunc)\n",
    "\n",
    "def join_mention(text):\n",
    "    return text.replace('@ ', '@')\n",
    "\n",
    "def tokenize(text):\n",
    "    return (word_tokenize(text))\n",
    "\n",
    "def process(content): \n",
    "    text = content\n",
    "    text = remove_hash(text)\n",
    "    text = remove_URL(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = join_mention(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!\n",
      "Donald Trump will be appearing on The View tomorrow morning to discuss Celebrity Apprentice and his new book Think Like A Champion!\n",
      "Donald Trump reads Top Ten Financial Tips on Late Show with David Letterman URL\n",
      "New Blog Post Celebrity Apprentice Finale and Lessons Learned Along the Way URL\n",
      "My persona will never be that of a wallflower  I’d rather build walls than cling to them Donald J. Trump\n"
     ]
    }
   ],
   "source": [
    "filepath = 'data/trumptweets.csv'\n",
    "\n",
    "data = pd.read_csv(filepath)\n",
    "# print(data.head())\n",
    "for i in range(5): \n",
    "    print(process(data.iloc[i]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12208    Great ruling on wind farm in Scotland—very sma...\n",
      "27415     MakeAmericaGreatAgain  Trump2016 LIFE CHANGIN...\n",
      "26208     @ellenEspence Im not convinced that any candi...\n",
      "26321    Dopey @GeorgeWill, a big proponent of the Iraq...\n",
      "1992     The election result in France is very disappoi...\n",
      "20920         @PerryGolf Top 100 Golf Courses @golfcom URL\n",
      "30495    VOTER REGISTRATION DEADLINES TODAY. You can re...\n",
      "23774    @absabella Mr.Trump speaks the TRUTH And Will ...\n",
      "8552      @wvcarsong Okay, please say hello to her for me.\n",
      "18481    Entrepreneurs Set the bar high. Do the best yo...\n",
      "Name: content_clean, dtype: object\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "sample = data.sample(n=10, random_state=42)\n",
    "\n",
    "sample['content_clean'] = sample.apply(lambda row: process(row['content']), axis=1)\n",
    "print(sample['content_clean'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_clean'] = data.apply(lambda row: process(row['content']), axis=1)\n",
    "data.to_csv('data/trumptweets_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "num_words = 20000 \n",
    "tokenizer = Tokenizer(num_words=num_words, filters='')\n",
    "\n",
    "tokenizer.fit_on_texts(data['content_clean'])\n",
    "encoded = tokenizer.texts_to_sequences(data['content_clean'])\n",
    "flat_encoded = [enc for encoder in encoded for enc in encoder]\n",
    "total_words = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
